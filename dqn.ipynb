{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from agents.dqn_agent import DQNAgent\n",
    "import env_configs\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import wrappers\n",
    "import numpy as np\n",
    "import torch\n",
    "from infrastructure import pytorch_util as ptu\n",
    "import tqdm\n",
    "\n",
    "from infrastructure import utils\n",
    "from infrastructure.logger import Logger\n",
    "from infrastructure.replay_buffer import MemoryEfficientReplayBuffer, ReplayBuffer\n",
    "\n",
    "from scripting_utils import make_logger, make_config\n",
    "\n",
    "MAX_NVIDEO = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  def __init__(self):\n",
    "    self.config_file = \"experiments/dqn/car_racing.yaml\"\n",
    "    self.metrics_interval = 10000\n",
    "    self.video_interval = 20000\n",
    "    self.save_interval = 10000\n",
    "    self.num_eval_trajectories = 10\n",
    "    self.num_render_trajectories = 1\n",
    "    self.seed = 1\n",
    "    self.no_gpu = False\n",
    "    self.which_gpu = 0\n",
    "    self.log_interval = 1000\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# create directory for logging\n",
    "logdir_prefix = \"hw3_dqn_\"  # keep for autograder\n",
    "\n",
    "config = make_config(args.config_file)\n",
    "logger = make_logger(logdir_prefix, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "ptu.init_gpu(use_gpu=not args.no_gpu, gpu_id=args.which_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the gym environment\n",
    "env = config[\"make_env\"]()\n",
    "eval_env = config[\"make_env\"]()\n",
    "render_env = config[\"make_env\"](render=True)\n",
    "\n",
    "exploration_schedule = config[\"exploration_schedule\"]\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "assert discrete, \"DQN only supports discrete action spaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "  env.observation_space.shape,\n",
    "  env.action_space.n,\n",
    "  **config[\"agent_kwargs\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation timestep, will be used for video saving\n",
    "if \"model\" in dir(env):\n",
    "  fps = 1 / env.model.opt.timestep\n",
    "elif \"render_fps\" in env.env.metadata:\n",
    "  fps = env.env.metadata[\"render_fps\"]\n",
    "else:\n",
    "  fps = 4\n",
    "\n",
    "ep_len = env.spec.max_episode_steps\n",
    "\n",
    "# Replay buffer\n",
    "if len(env.observation_space.shape) == 3:\n",
    "  print(\"using memory-efficient replay buffer\")\n",
    "  stacked_frames = True\n",
    "  frame_history_len = env.observation_space.shape[0]\n",
    "  assert frame_history_len == 4, \"only support 4 stacked frames\"\n",
    "  replay_buffer = MemoryEfficientReplayBuffer(\n",
    "    frame_history_len=frame_history_len\n",
    "  )\n",
    "elif len(env.observation_space.shape) == 1:\n",
    "  print(\"using normal replay buffer\")\n",
    "  stacked_frames = False\n",
    "  replay_buffer = ReplayBuffer()\n",
    "else:\n",
    "  raise ValueError(\n",
    "    f\"Unsupported observation space shape: {env.observation_space.shape}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env_training():\n",
    "  # nonlocal observation\n",
    "\n",
    "  observation, info = env.reset()\n",
    "\n",
    "  # assert not isinstance(\n",
    "  #   observation, tuple\n",
    "  # ), \"env.reset() must return np.ndarray - make sure your Gym version uses the old step API\"\n",
    "  \n",
    "  observation = np.asarray(observation)\n",
    "  \n",
    "  if isinstance(replay_buffer, MemoryEfficientReplayBuffer):\n",
    "    replay_buffer.on_reset(observation=observation[-1, ...])\n",
    "\n",
    "  return observation, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = reset_env_training()\n",
    "\n",
    "for step in tqdm.trange(1, config[\"total_steps\"]+1, dynamic_ncols=True):\n",
    "  # step += past_step # for resuming training\n",
    "\n",
    "  epsilon = exploration_schedule.value(step)\n",
    "  \n",
    "  # Compute action\n",
    "  action = agent.get_action(observation, epsilon)\n",
    "\n",
    "  # Step the environment\n",
    "  next_observation, reward, terminated, truncated, info = env.step(action) # done got replaced by terminated and truncated\n",
    "  next_observation = np.asarray(next_observation)\n",
    "\n",
    "  # TODO(student): Add the data to the replay buffer\n",
    "  if isinstance(replay_buffer, MemoryEfficientReplayBuffer):\n",
    "    # We're using the memory-efficient replay buffer,\n",
    "    # so we only insert next_observation (not observation)\n",
    "    if not stacked_frames:\n",
    "      print(\"WARNING: Stacked frames not enabled, but using memory-efficient replay buffer\")\n",
    "\n",
    "    replay_buffer.insert(\n",
    "      action=action,\n",
    "      reward=reward,\n",
    "      next_observation=next_observation[-1], # if stacked_frames else next_observation, # Only insert the last frame (frame stacking)\n",
    "      terminated=terminated,\n",
    "    )\n",
    "\n",
    "  else:\n",
    "    # We're using the regular replay buffer\n",
    "    replay_buffer.insert(\n",
    "      observation=observation,\n",
    "      action=action,\n",
    "      reward=reward,\n",
    "      next_observation=next_observation,\n",
    "      terminated=terminated,\n",
    "    )\n",
    "\n",
    "  # Handle episode termination\n",
    "  if terminated or truncated:\n",
    "    observation, info = reset_env_training()\n",
    "\n",
    "    if \"episode\" in info:\n",
    "      logger.log_scalar(info[\"episode\"][\"r\"], \"train_return\", step)\n",
    "      logger.log_scalar(info[\"episode\"][\"l\"], \"train_ep_len\", step)\n",
    "  else:\n",
    "    observation = next_observation\n",
    "\n",
    "  # Main DQN training loop\n",
    "  if step >= config[\"learning_starts\"]:\n",
    "    # TODO(student): Sample config[\"batch_size\"] samples from the replay buffer\n",
    "    batch = replay_buffer.sample(config[\"batch_size\"])\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    batch = ptu.from_numpy(batch)\n",
    "\n",
    "    # TODO(student): Train the agent. `batch` is a dictionary of numpy arrays,\n",
    "    update_info = agent.update(\n",
    "      batch[\"observations\"],\n",
    "      batch[\"actions\"],\n",
    "      batch[\"rewards\"],\n",
    "      batch[\"next_observations\"],\n",
    "      batch[\"terminateds\"],\n",
    "      step,\n",
    "    )\n",
    "\n",
    "    # Logging code\n",
    "    update_info[\"epsilon\"] = epsilon\n",
    "    update_info[\"lr\"] = agent.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "    if step % args.log_interval == 0:\n",
    "      for k, v in update_info.items():\n",
    "        logger.log_scalar(v, k, step)\n",
    "      logger.flush()\n",
    "\n",
    "  if step % args.save_interval == 0:\n",
    "    agent.save(os.path.join(logger._log_dir, f\"agent_{step}.pt\"))\n",
    "\n",
    "  if step % args.metrics_interval == 0:\n",
    "    # Evaluate\n",
    "    trajectories = utils.sample_n_trajectories(\n",
    "      eval_env,\n",
    "      agent,\n",
    "      args.num_eval_trajectories,\n",
    "      ep_len,\n",
    "    )\n",
    "    returns = [t[\"episode_statistics\"][\"r\"] for t in trajectories]\n",
    "    ep_lens = [t[\"episode_statistics\"][\"l\"] for t in trajectories]\n",
    "\n",
    "    logger.log_scalar(np.mean(returns), \"eval_return\", step)\n",
    "    logger.log_scalar(np.mean(ep_lens), \"eval_ep_len\", step)\n",
    "\n",
    "    if len(returns) > 1:\n",
    "      logger.log_scalar(np.std(returns), \"eval/return_std\", step)\n",
    "      logger.log_scalar(np.max(returns), \"eval/return_max\", step)\n",
    "      logger.log_scalar(np.min(returns), \"eval/return_min\", step)\n",
    "      logger.log_scalar(np.std(ep_lens), \"eval/ep_len_std\", step)\n",
    "      logger.log_scalar(np.max(ep_lens), \"eval/ep_len_max\", step)\n",
    "      logger.log_scalar(np.min(ep_lens), \"eval/ep_len_min\", step)\n",
    "\n",
    "  if args.num_render_trajectories > 0 and step % args.video_interval == 0:\n",
    "    video_trajectories = utils.sample_n_trajectories(\n",
    "      render_env,\n",
    "      agent,\n",
    "      args.num_render_trajectories,\n",
    "      ep_len,\n",
    "      render=True,\n",
    "    )\n",
    "\n",
    "    logger.log_paths_as_videos(\n",
    "      video_trajectories,\n",
    "      step,\n",
    "      fps=fps,\n",
    "      max_videos_to_save=args.num_render_trajectories,\n",
    "      video_title=\"eval_rollouts\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
