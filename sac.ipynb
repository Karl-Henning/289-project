{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from agents.soft_actor_critic import SoftActorCritic\n",
    "from infrastructure.replay_buffer import ReplayBuffer\n",
    "import env_configs\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import wrappers\n",
    "import numpy as np\n",
    "import torch\n",
    "from infrastructure import pytorch_util as ptu\n",
    "import tqdm\n",
    "\n",
    "from infrastructure import utils\n",
    "from infrastructure.logger import Logger\n",
    "\n",
    "from scripting_utils import make_logger, make_config\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  def __init__(self):\n",
    "    self.config_file = \"experiments/sac/halfcheetah_reparametrize.yaml\"\n",
    "    self.eval_interval = 5000\n",
    "    self.num_eval_trajectories = 10\n",
    "    self.num_render_trajectories = 0\n",
    "    self.seed = 1\n",
    "    self.no_gpu = False\n",
    "    self.which_gpu = 0\n",
    "    self.log_interval = 1000\n",
    "\n",
    "args = Args()\n",
    "        \n",
    "\n",
    "# create directory for logging\n",
    "logdir_prefix = \"hw3_sac_\"  # keep for autograder\n",
    "\n",
    "config = make_config(args.config_file)\n",
    "logger = make_logger(logdir_prefix, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "ptu.init_gpu(use_gpu=not args.no_gpu, gpu_id=args.which_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the gym environment\n",
    "env = config[\"make_env\"]()\n",
    "eval_env = config[\"make_env\"]()\n",
    "render_env = config[\"make_env\"](render=True)\n",
    "\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "assert (\n",
    "  not discrete\n",
    "), \"Our actor-critic implementation only supports continuous action spaces. (This isn't a fundamental limitation, just a current implementation decision.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_shape = env.observation_space.shape\n",
    "ac_dim = env.action_space.shape[0]\n",
    "\n",
    "# initialize agent\n",
    "agent = SoftActorCritic(\n",
    "  ob_shape,\n",
    "  ac_dim,\n",
    "  **config[\"agent_kwargs\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation timestep, will be used for video saving\n",
    "if \"model\" in dir(env):\n",
    "  fps = 1 / env.model.opt.timestep\n",
    "else:\n",
    "  fps = env.env.metadata[\"render_fps\"]\n",
    "\n",
    "ep_len = config[\"ep_len\"] or env.spec.max_episode_steps\n",
    "batch_size = config[\"batch_size\"] # or batch_size\n",
    "\n",
    "replay_buffer = ReplayBuffer(config[\"replay_buffer_capacity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "for step in tqdm.trange(config[\"total_steps\"], dynamic_ncols=True):\n",
    "  if step < config[\"random_steps\"]:\n",
    "    action = env.action_space.sample()\n",
    "  else:\n",
    "    # TODO(student): Select an action\n",
    "    action = agent.get_action(observation)\n",
    "\n",
    "  # Step the environment and add the data to the replay buffer\n",
    "  next_observation, reward, terminated, truncated, info = env.step(action) # done got replaced by terminated and truncated\n",
    "  next_observation = np.asarray(next_observation)\n",
    "\n",
    "  truncated = info.get(\"TimeLimit.truncated\", False)\n",
    "  replay_buffer.insert(\n",
    "    observation=observation,\n",
    "    action=action,\n",
    "    reward=reward,\n",
    "    next_observation=next_observation,\n",
    "    terminated=terminated,\n",
    "  )\n",
    "\n",
    "  if terminated or truncated:\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    if \"episode\" in info:\n",
    "      logger.log_scalar(info[\"episode\"][\"r\"], \"train_return\", step)\n",
    "      logger.log_scalar(info[\"episode\"][\"l\"], \"train_ep_len\", step)\n",
    "  else:\n",
    "    observation = next_observation\n",
    "\n",
    "  # Train the agent\n",
    "  if step >= config[\"training_starts\"]:\n",
    "    # TODO(student): Sample a batch of config[\"batch_size\"] transitions from the replay buffer\n",
    "    batch = replay_buffer.sample(config[\"batch_size\"])\n",
    "    batch =  ptu.from_numpy(batch)\n",
    "    update_info = agent.update(\n",
    "      batch[\"observations\"],\n",
    "      batch[\"actions\"],\n",
    "      batch[\"rewards\"],\n",
    "      batch[\"next_observations\"],\n",
    "      batch[\"terminateds\"],\n",
    "      step,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Logging\n",
    "    update_info[\"actor_lr\"] = agent.actor_lr_scheduler.get_last_lr()[0]\n",
    "    update_info[\"critic_lr\"] = agent.critic_lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "    if step % args.log_interval == 0:\n",
    "      for k, v in update_info.items():\n",
    "        logger.log_scalar(v, k, step)\n",
    "        logger.log_scalars\n",
    "      logger.flush()\n",
    "\n",
    "  # Run evaluation\n",
    "  if step % args.eval_interval == 0:\n",
    "    trajectories = utils.sample_n_trajectories(\n",
    "      eval_env,\n",
    "      policy=agent,\n",
    "      ntraj=args.num_eval_trajectories,\n",
    "      max_length=ep_len,\n",
    "    )\n",
    "    returns = [t[\"episode_statistics\"][\"r\"] for t in trajectories]\n",
    "    ep_lens = [t[\"episode_statistics\"][\"l\"] for t in trajectories]\n",
    "\n",
    "    logger.log_scalar(np.mean(returns), \"eval_return\", step)\n",
    "    logger.log_scalar(np.mean(ep_lens), \"eval_ep_len\", step)\n",
    "\n",
    "    if len(returns) > 1:\n",
    "      logger.log_scalar(np.std(returns), \"eval/return_std\", step)\n",
    "      logger.log_scalar(np.max(returns), \"eval/return_max\", step)\n",
    "      logger.log_scalar(np.min(returns), \"eval/return_min\", step)\n",
    "      logger.log_scalar(np.std(ep_lens), \"eval/ep_len_std\", step)\n",
    "      logger.log_scalar(np.max(ep_lens), \"eval/ep_len_max\", step)\n",
    "      logger.log_scalar(np.min(ep_lens), \"eval/ep_len_min\", step)\n",
    "\n",
    "    if args.num_render_trajectories > 0:\n",
    "      video_trajectories = utils.sample_n_trajectories(\n",
    "        render_env,\n",
    "        agent,\n",
    "        args.num_render_trajectories,\n",
    "        ep_len,\n",
    "        render=True,\n",
    "      )\n",
    "\n",
    "      logger.log_paths_as_videos(\n",
    "        video_trajectories,\n",
    "        step,\n",
    "        fps=fps,\n",
    "        max_videos_to_save=args.num_render_trajectories,\n",
    "        video_title=\"eval_rollouts\",\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-testing-jmrYQwNM-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
